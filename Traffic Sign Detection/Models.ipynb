{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PivjitYKv9nI",
        "outputId": "353223c4-24c2-4f94-9d4c-9ee39355ebf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_xH5RmU9su9W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as td\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import cv2\n",
        "from torchmetrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o01h9AVJdAur",
        "outputId": "2ebb60e5-1445-4b7c-bdf2-ae140b8ad525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZ-iCxjdDJf",
        "outputId": "5e6f6abb-0391-4c16-dc9b-aa37ec1e2e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Indian Traffic Signs Prediction(85 classes)', 'Persian Traffic Sign Dataset (PTSD)', 'Traffic Signs (GTSRB plus 162 custom classes)']\n"
          ]
        }
      ],
      "source": [
        "# path='/content/drive/My Drive/6721 Dataset/6721 Project dataset/'\n",
        "path='/content/drive/My Drive/6721 Project dataset/'\n",
        "path_save = '/content/drive/My Drive/6721 Dataset/'\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ud-55TXwjopF"
      },
      "outputs": [],
      "source": [
        "path_dataset1 = path+\"Indian Traffic Signs Prediction(85 classes)/\"\n",
        "path_dataset2 = path+\"Persian Traffic Sign Dataset (PTSD)/\"\n",
        "path_dataset3 = path+\"Traffic Signs (GTSRB plus 162 custom classes)/Data_images/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbmXHVFl-LrR"
      },
      "source": [
        "**Data Loaders**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nAjOrIQaaACu"
      },
      "outputs": [],
      "source": [
        "def load_data(path_train, val_split, path_test, batch_size, input_size):\n",
        "  \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    transform_train= transforms.Compose([transforms.Resize(input_size),\n",
        "                                          #transforms.ColorJitter(brightness=(0.8,1.2), contrast=None, saturation=None, hue=None),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          normalize\n",
        "                                          ])\n",
        "\n",
        "    transform_test = transforms.Compose([transforms.Resize(input_size),\n",
        "                                         transforms.ToTensor(), \n",
        "                                         normalize])\n",
        "\n",
        "    data_train = datasets.ImageFolder(root=path_train, transform=transform_train)\n",
        "    data_test = datasets.ImageFolder(root=path_test, transform=transform_test)\n",
        "    \n",
        "\n",
        "    mappings = data_train.class_to_idx\n",
        "\n",
        "    val_size = int(len(data_train)*val_split)\n",
        "    train_size = len(data_train) - val_size\n",
        "\n",
        "    train_dataset, val_dataset = td.random_split(data_train, [train_size, val_size])\n",
        "    \n",
        "    # class_weights = []\n",
        "    # for root, subdir, files in os.walk(path_train):\n",
        "    #     if len(files) > 0:\n",
        "    #         class_weights.append(1/len(files))\n",
        "\n",
        "    # sample_weights = [0] * len(data_train)\n",
        "\n",
        "    # for idx, (data, label) in enumerate(data_train):\n",
        "    #     class_weight = class_weights[label]\n",
        "    #     sample_weights[idx] = class_weight\n",
        "\n",
        "    # sampler = td.WeightedRandomSampler(sample_weights, num_samples=\n",
        "    #                                 len(sample_weights), replacement=True)\n",
        "    \n",
        "    data_loader_train = td.DataLoader(train_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      drop_last=False,\n",
        "                                      num_workers=0,       \n",
        "                                      pin_memory=True)\n",
        "     \n",
        "    data_loader_val = td.DataLoader(val_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    drop_last=False,\n",
        "                                    num_workers=0) \n",
        "      \n",
        "    data_loader_test = td.DataLoader(data_test,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   drop_last=False,\n",
        "                                   num_workers=0)\n",
        "    \n",
        "    return data_loader_train, data_loader_test, data_loader_val, mappings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Class Weights"
      ],
      "metadata": {
        "id": "cTWOhhFV1kuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_size_dist(path):\n",
        "    t1 = time.time()\n",
        "\n",
        "    folders = os.listdir(path)\n",
        "    total = 0\n",
        "\n",
        "    class_samples = {}\n",
        "\n",
        "    for folder in folders:\n",
        "        samples = os.listdir(path+folder)\n",
        "\n",
        "        class_samples[folder]=len(samples)\n",
        "\n",
        "        print(\"Class: {} has samples: {} \".format(folder,len(samples)))\n",
        "        total+=len(samples)\n",
        "\n",
        "    print(\"\\nTotal number of samples: \",total)\n",
        "\n",
        "    print(\"\\nTime taken: {}\".format(time.time()-t1))\n",
        "\n",
        "    return class_samples, total"
      ],
      "metadata": {
        "id": "AybvEo1V1nLP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(path_train, mappings):\n",
        "\n",
        "    class_samples, total = get_image_size_dist(path_train)\n",
        "\n",
        "    class_weights = [0]*len(mappings.keys())\n",
        "\n",
        "    for key in mappings.keys():\n",
        "        class_weights[mappings[key]] = 1 - (class_samples[key]/ total)\n",
        "      \n",
        "    return class_weights"
      ],
      "metadata": {
        "id": "cs77yc_J1m94"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ystywhe-rXn"
      },
      "source": [
        "Training Setup\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Zz46kldS13If"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "\n",
        "def get_criterion(class_weights):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "    return criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "s9lpiTqe-y49"
      },
      "outputs": [],
      "source": [
        "def train_model(model, num_epochs, train_loader, criterion, optimizer, savepath, device):\n",
        "\n",
        "      total_steps = len(train_loader)\n",
        "\n",
        "      t1 = time.time()\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          for i, data in enumerate(train_loader):\n",
        "              \n",
        "              images, labels = data[0].to(device), data[1].to(device)\n",
        "              \n",
        "              model.train()\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(images)\n",
        "              loss = criterion(outputs, labels)\n",
        "              \n",
        "              # Backprop and optimisation\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "              # Train accuracy\n",
        "              total = labels.size(0)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              correct = (predicted == labels).sum().item()\n",
        "              \n",
        "              if (i + 1) % 10 == 0:\n",
        "                  model.eval() \n",
        "                  with torch.no_grad(): \n",
        "                      correctv = 0\n",
        "                      totalv = 0\n",
        "                      for datav in val_loader:\n",
        "                          imagesv, labelsv = datav[0].to(device), datav[1].to(device)\n",
        "                          outputsv = model(imagesv)\n",
        "                          _, predictedv = torch.max(outputsv.data, 1)\n",
        "                          totalv += labelsv.size(0)\n",
        "                          correctv += (predictedv == labelsv).sum().item()\n",
        "                          \n",
        "                      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'\n",
        "                      .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
        "                          (correct / total) * 100,\n",
        "                          (correctv / totalv) * 100))\n",
        "                      \n",
        "                  \n",
        "      print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n",
        "\n",
        "      print(\"/n/n Saving model at: \",savepath)\n",
        "      torch.save(model.state_dict(), savepath)\n",
        "\n",
        "      return model, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_mHsQq5I-yyu"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device,nclasses):\n",
        "   \n",
        "    model.eval() \n",
        "\n",
        "    Y=[]\n",
        "    y=[]\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            Y.extend(labels)\n",
        "            y.extend(predicted)\n",
        "\n",
        "        acc = (correct / total) * 100\n",
        "        f1 = F1Score(num_classes=nclasses)\n",
        "        f1score = f1(torch.IntTensor(y),torch.IntTensor(Y))\n",
        "\n",
        "        print('Model Evaluation Results on {} test samples'.format(total))\n",
        "        print('Test Accuracy: ', acc)\n",
        "        print('Test F1 Score: ',f1score)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbDH531X-anJ"
      },
      "source": [
        "Architecture 1: AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_53CBsNT7_Ds"
      },
      "source": [
        "On dataset 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bmokcdO8N-v0"
      },
      "outputs": [],
      "source": [
        "nclasses=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0C0mVSFA81I",
        "outputId": "301594ca-ea4d-4245-978e-5013b6aaaaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ]
        }
      ],
      "source": [
        "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gUhYh2APBCmT"
      },
      "outputs": [],
      "source": [
        "prev_out = AlexNet_model.classifier[4].out_features\n",
        "AlexNet_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTMSRbBQBF4h",
        "outputId": "763e97b3-aba5-40d9-c549-e3cda17775b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "AlexNet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yKGRy1ztBL28"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(AlexNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dnyRBBLrs23v"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset1+\"train/\"\n",
        "path_test = path_dataset1+\"test/\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {}\".format(device))\n",
        "AlexNet_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQw6FaosKl2Y",
        "outputId": "95c253ce-c144-4af0-f947-85769906d0e5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "\n",
        "class_weights = torch.FloatTensor(class_weights)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi0ZJxXd6S4B",
        "outputId": "a9b5d68d-4ca4-417e-e5ba-ba6aff709b51"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: CROSS_ROAD has samples: 140 \n",
            "Class: GAP_IN_MEDIAN has samples: 180 \n",
            "Class: COMPULSARY_KEEP_RIGHT has samples: 223 \n",
            "Class: HORN_PROHIBITED has samples: 160 \n",
            "Class: HUMP_OR_ROUGH_ROAD has samples: 101 \n",
            "Class: LEFT_TURN_PROHIBITED has samples: 126 \n",
            "Class: NO_ENTRY has samples: 174 \n",
            "Class: NO_STOPPING_OR_STANDING has samples: 242 \n",
            "Class: PEDESTRIAN_CROSSING has samples: 121 \n",
            "Class: SPEED_LIMIT_40 has samples: 170 \n",
            "Class: SPEED_LIMIT_80 has samples: 192 \n",
            "Class: SPEED_LIMIT_30 has samples: 238 \n",
            "Class: SPEED_LIMIT_50 has samples: 200 \n",
            "Class: SPEED_LIMIT_60 has samples: 190 \n",
            "Class: SPEED_LIMIT_70 has samples: 160 \n",
            "\n",
            "Total number of samples:  2617\n",
            "\n",
            "Time taken: 0.049219369888305664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "pzxEFoxxs26O",
        "outputId": "6f864cbf-1992-490a-8399-d631e7b385e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-4046420ca032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msavepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_save\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"alexnetmodel_d1.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrained_AlexNet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAlexNet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-c93783827f39>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, train_loader, criterion, optimizer, savepath, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "savepath = path_save+\"alexnetmodel_d1.pt\"\n",
        "\n",
        "trained_AlexNet_model, device = train_model(AlexNet_model, num_epochs, train_loader, criterion, optimizer, savepath, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBVfPGxuqTaY"
      },
      "outputs": [],
      "source": [
        "# for loading model\n",
        "# trained_AlexNet_model.load_state_dict(torch.load(path+\"alexnetmodel.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnfIndd2s29e",
        "outputId": "3e4bbc6c-d292-48f7-ab90-c65c149acd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 710 test images: 19.859154929577468 %\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(trained_AlexNet_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUru4h3JFq2b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEqLQu1V8O9v"
      },
      "source": [
        "On dataset 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8TAKHdJN-v3"
      },
      "outputs": [],
      "source": [
        "nclasses=12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2j6kffT8O9v",
        "outputId": "c5193785-0b42-427d-9827-29d6fc373ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrZajJzb8O9v"
      },
      "outputs": [],
      "source": [
        "prev_out = AlexNet_model.classifier[4].out_features\n",
        "AlexNet_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLV6Cd7r8O9w"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(AlexNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xjiDwAq8O9w",
        "outputId": "9e5b9e13-3cc0-4ea5-94e2-7440b7250449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.007142857142857143, 0.005555555555555556, 0.004484304932735426, 0.00625, 0.009900990099009901, 0.007936507936507936, 0.005747126436781609, 0.004132231404958678, 0.008264462809917356, 0.0058823529411764705, 0.005208333333333333, 0.004201680672268907, 0.005, 0.005263157894736842, 0.00625]\n",
            "[0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
            "Time taken\n"
          ]
        }
      ],
      "source": [
        "path_train = path_dataset2+\"train/\"\n",
        "path_test = path_dataset2+\"test/\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "HOwQJQNM9fre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQGp2Bk38O9w",
        "outputId": "260ac207-57cc-470f-f9b4-b6699a0cc698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "Epoch [1/1], Step [10/66], Loss: 2.7568, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [20/66], Loss: 2.6704, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [30/66], Loss: 2.6873, Accuracy: 9.38%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [40/66], Loss: 2.6864, Accuracy: 15.62%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [50/66], Loss: 2.6760, Accuracy: 3.12%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [60/66], Loss: 2.5726, Accuracy: 15.62%, Validation Accuracy: 15.30%\n",
            "######## Training Finished in 235.3470482826233 seconds ###########\n"
          ]
        }
      ],
      "source": [
        "savepath = path_save+\"alexnetmodel_d2.pt\"\n",
        "\n",
        "trained_AlexNet_model, device = train_model(AlexNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qdjOT2K8O9w"
      },
      "outputs": [],
      "source": [
        "# for loading model\n",
        "# trained_AlexNet_model.load_state_dict(torch.load(path+\"alexnetmodel.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UEq0bPW8O9w",
        "outputId": "3e4bbc6c-d292-48f7-ab90-c65c149acd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 710 test images: 19.859154929577468 %\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(trained_AlexNet_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSLvQhsB9mz5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvla9-oK9d45"
      },
      "source": [
        "On dataset 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byAB4YZqN-v5"
      },
      "outputs": [],
      "source": [
        "nclasses=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR81ilBI9d46",
        "outputId": "c5193785-0b42-427d-9827-29d6fc373ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "239Iqf0a9d46"
      },
      "outputs": [],
      "source": [
        "prev_out = AlexNet_model.classifier[4].out_features\n",
        "AlexNet_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji7tYYGa9d46"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(AlexNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QAcirVL9d46",
        "outputId": "9e5b9e13-3cc0-4ea5-94e2-7440b7250449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.007142857142857143, 0.005555555555555556, 0.004484304932735426, 0.00625, 0.009900990099009901, 0.007936507936507936, 0.005747126436781609, 0.004132231404958678, 0.008264462809917356, 0.0058823529411764705, 0.005208333333333333, 0.004201680672268907, 0.005, 0.005263157894736842, 0.00625]\n",
            "[0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.007142857142857143, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.005555555555555556, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.009900990099009901, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.007936507936507936, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.005747126436781609, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.004132231404958678, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.008264462809917356, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.0058823529411764705, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.005208333333333333, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.004201680672268907, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.005263157894736842, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
            "Time taken\n"
          ]
        }
      ],
      "source": [
        "path_train = path_dataset3+\"Train/\"\n",
        "path_test = path_dataset3+\"Test/\"\n",
        "\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "grcNm8jK9g-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqZz0MtR9d46",
        "outputId": "260ac207-57cc-470f-f9b4-b6699a0cc698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "Epoch [1/1], Step [10/66], Loss: 2.7568, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [20/66], Loss: 2.6704, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [30/66], Loss: 2.6873, Accuracy: 9.38%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [40/66], Loss: 2.6864, Accuracy: 15.62%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [50/66], Loss: 2.6760, Accuracy: 3.12%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [60/66], Loss: 2.5726, Accuracy: 15.62%, Validation Accuracy: 15.30%\n",
            "######## Training Finished in 235.3470482826233 seconds ###########\n"
          ]
        }
      ],
      "source": [
        "savepath = path_save+\"alexnetmodel_d3.pt\"\n",
        "\n",
        "trained_AlexNet_model, device = train_model(AlexNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FjXkk_P9d46"
      },
      "outputs": [],
      "source": [
        "# for loading model\n",
        "# trained_AlexNet_model.load_state_dict(torch.load(path+\"alexnetmodel.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U8CCcMO9d46",
        "outputId": "3e4bbc6c-d292-48f7-ab90-c65c149acd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 710 test images: 19.859154929577468 %\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(trained_AlexNet_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Oy50W5vP6I"
      },
      "source": [
        "Architecture 2: VGG-11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m7fXEW-QhP"
      },
      "source": [
        "on dataset 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBbARVON-v7"
      },
      "outputs": [],
      "source": [
        "nclasses=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbQ_Vv-ZvS5D",
        "outputId": "e8d31eed-016d-4cc5-9515-d81db58fc832"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "VGG11_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CICeGKI3vjDw"
      },
      "outputs": [],
      "source": [
        "prev_out = VGG11_model.classifier[3].out_features\n",
        "VGG11_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2b6wbT-vy9o",
        "outputId": "00ac485b-9ba2-4d0c-f61c-878d21bae4cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VGG11_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgOzrKXZv3DL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(VGG11_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdrYPPhv_06"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset1+\"train\"\n",
        "path_test = path_dataset1+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "SxOUN1269iIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VBebkPiwIuh"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"vgg11model_d1.pt\"\n",
        "trained_VGG11_model, device = train_model(VGG11_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDj3U6BXwQYn"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_VGG11_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT1TCKuJ-aU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Oow3Xp-aup"
      },
      "source": [
        "on dataset 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCvYyvJgN-wC"
      },
      "outputs": [],
      "source": [
        "nclasses=12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPB26YC7-aup",
        "outputId": "e8d31eed-016d-4cc5-9515-d81db58fc832"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "VGG11_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-6iOeEd-aup"
      },
      "outputs": [],
      "source": [
        "prev_out = VGG11_model.classifier[3].out_features\n",
        "VGG11_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6GC9UD3-auq"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(VGG11_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZfAJgf-auq"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset2+\"train\"\n",
        "path_test = path_dataset2+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "2soll5hk9jMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPQV9EqR-auq"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"vgg11model_d2.pt\"\n",
        "trained_VGG11_model, device = train_model(VGG11_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOfbMdw1-auq"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_VGG11_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcpBBX-j-j25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE0rLx9_-kGI"
      },
      "source": [
        "on dataset 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWfOdWWvN-wD"
      },
      "outputs": [],
      "source": [
        "nclasses=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5mLp8L_-kGI",
        "outputId": "0d65bd7e-4225-4946-dfc0-2c1a1827fb77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "VGG11_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Zx3OReu-kGI"
      },
      "outputs": [],
      "source": [
        "prev_out = VGG11_model.classifier[3].out_features\n",
        "VGG11_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgN9LW3w-kGI"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(VGG11_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "kanuR4xY-kGJ",
        "outputId": "1bcba480-be65-4893-8391-dc4dea009340"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-27ab25fc4d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6fe5488cb045>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path_train, val_split, path_test, batch_size, input_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m                                          normalize])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Note that scandir is global in this module due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# to earlier import-*.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mscandir_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monerror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "path_train = path_dataset3+\"Train/\"\n",
        "path_test = path_dataset3+\"Test/\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "ETbofcER9kc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0-mcNJ--kGJ"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"vgg11model_d3.pt\"\n",
        "trained_VGG11_model, device = train_model(VGG11_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTFDbEzh-kGJ"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_VGG11_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QGSudN3rfQ"
      },
      "source": [
        "Architecture 3: ResNet-18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykyHhgTj_7gY"
      },
      "source": [
        "on dataset 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbxKzoP2N-wF"
      },
      "outputs": [],
      "source": [
        "nclasses=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjeyUpF53r6L",
        "outputId": "3245f470-5281-4267-8f79-e8e23d7c8790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "ResNet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZz850Gk4P_f"
      },
      "outputs": [],
      "source": [
        "ResNet_model.fc = nn.Linear( 512, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1niUp7-3ucX",
        "outputId": "46b7d327-13e0-4e2f-b744-5b28b279b71b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ResNet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAE_uMea3uY1"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoNH0dSl3uVq"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset1+\"train\"\n",
        "path_test = path_dataset1+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "z-cPhH5N9lrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alEPE-YH3uSY"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"resnetmodel_d1.pt\"\n",
        "trained_ResNet_model, device = train_model(ResNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-r7XF9a3uPx"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_ResNet_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaEWAeB-__Hp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mw6pKMU__lE"
      },
      "source": [
        "on dataset 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR81Mz3_N-wG"
      },
      "outputs": [],
      "source": [
        "nclasses=12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gufOXKd1__lE",
        "outputId": "3245f470-5281-4267-8f79-e8e23d7c8790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "ResNet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXeQGpgi__lF"
      },
      "outputs": [],
      "source": [
        "ResNet_model.fc = nn.Linear( 512, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE8BaxQX__lF"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWlFjNQb__lF"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset2+\"train\"\n",
        "path_test = path_dataset2+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "JTKsc7J49mpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ERflED__lF"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"resnetmodel_d2.pt\"\n",
        "trained_ResNet_model, device = train_model(ResNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mxR4zp3__lF"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_ResNet_model, test_loader, device, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3pzq0u1ASD_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4043AzG3ASUy"
      },
      "source": [
        "on dataset 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slCE7VT-N-wI"
      },
      "outputs": [],
      "source": [
        "nclasses=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0Ur9P8HASUz",
        "outputId": "3245f470-5281-4267-8f79-e8e23d7c8790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ],
      "source": [
        "ResNet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrfStAaKASUz"
      },
      "outputs": [],
      "source": [
        "ResNet_model.fc = nn.Linear( 512, nclasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBvyHzXGASUz"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEjpaUpcASUz"
      },
      "outputs": [],
      "source": [
        "path_train = path_dataset3+\"Train/\"\n",
        "path_test = path_dataset3+\"Test/\"\n",
        "val_split = 0.2\n",
        "batch_size = 128\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader, mappings = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(path_train, mappings)\n",
        "criterion = get_criterion(class_weights)"
      ],
      "metadata": {
        "id": "1uuIdfbD9nnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSolqegyASUz"
      },
      "outputs": [],
      "source": [
        "savepath = path_save+\"resnetmodel_d3.pt\"\n",
        "trained_ResNet_model, device = train_model(ResNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhsEbu7AASUz"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trained_ResNet_model, test_loader, device, nclasses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}