{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xH5RmU9su9W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as td\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import cv2\n",
        "from torchmetrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o01h9AVJdAur",
        "outputId": "edc6d89f-ca74-466d-f72f-b3bf1e611113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/My Drive/6721 Project dataset/'\n",
        "print(os.listdir(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZ-iCxjdDJf",
        "outputId": "e6acfc9f-73f6-4764-dfc0-36388118f4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Indian Traffic Signs Prediction(85 classes)', 'Persian Traffic Sign Dataset (PTSD)', 'Traffic Signs (GTSRB plus 162 custom classes)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_dataset1 = path+\"Indian Traffic Signs Prediction(85 classes)/\"\n",
        "path_dataset2 = path+\"Persian Traffic Sign Dataset (PTSD)/\"\n",
        "path_dataset3 = path+\"Traffic Signs (GTSRB plus 162 custom classes)/Data_images/\""
      ],
      "metadata": {
        "id": "ud-55TXwjopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lOYP1i7G-HCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather original number of classes in each dataset"
      ],
      "metadata": {
        "id": "MpAT6DmNCaRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_size_dist(path):\n",
        "    t1 = time.time()\n",
        "\n",
        "    folders = os.listdir(path)\n",
        "\n",
        "    for folder in folders:\n",
        "\n",
        "        img_x=[]\n",
        "        img_y=[]\n",
        "\n",
        "        samples = os.listdir(path+folder)\n",
        "\n",
        "        # for sample in samples:\n",
        "            # img = cv2.imread(path+folder+\"/\"+sample)\n",
        "\n",
        "            # img_x.append(img.shape[0])\n",
        "            # img_y.append(img.shape[1])\n",
        "\n",
        "        print(\"Class: {} has samples: {} \".format(folder,len(samples)))\n",
        "        # plt.scatter(img_x, img_y)\n",
        "        # plt.show()\n",
        "\n",
        "    print(\"\\nTime taken: {}\".format(time.time()-t1))"
      ],
      "metadata": {
        "id": "b6a-CdLjG4FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 1: Indian Traffic Signs Prediction(85 classes)"
      ],
      "metadata": {
        "id": "rSXVxfBDBLoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = path_dataset1+\"train/\"\n",
        "test_path = path_dataset1+\"test/\""
      ],
      "metadata": {
        "id": "GkEmNBhn-K7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(train_path)"
      ],
      "metadata": {
        "id": "7pXpkORWHVMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af0345a-92d3-4c3d-9640-ca0ca850501d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: CROSS_ROAD has samples: 140 \n",
            "Class: GAP_IN_MEDIAN has samples: 180 \n",
            "Class: COMPULSARY_KEEP_RIGHT has samples: 223 \n",
            "Class: HORN_PROHIBITED has samples: 160 \n",
            "Class: HUMP_OR_ROUGH_ROAD has samples: 101 \n",
            "Class: LEFT_TURN_PROHIBITED has samples: 126 \n",
            "Class: NO_ENTRY has samples: 174 \n",
            "Class: NO_STOPPING_OR_STANDING has samples: 242 \n",
            "Class: PEDESTRIAN_CROSSING has samples: 121 \n",
            "Class: SPEED_LIMIT_40 has samples: 170 \n",
            "Class: SPEED_LIMIT_80 has samples: 192 \n",
            "Class: SPEED_LIMIT_30 has samples: 238 \n",
            "Class: SPEED_LIMIT_50 has samples: 200 \n",
            "Class: SPEED_LIMIT_60 has samples: 190 \n",
            "Class: SPEED_LIMIT_70 has samples: 160 \n",
            "\n",
            "Time taken: 0.04852437973022461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(test_path)"
      ],
      "metadata": {
        "id": "YdojIO8xHVCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 2: Persian Traffic Sign Dataset (PTSD)"
      ],
      "metadata": {
        "id": "coCbrEWaBSpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = path_dataset2+\"train/\"\n",
        "test_path = path_dataset2+\"test/\""
      ],
      "metadata": {
        "id": "M1WRPcW3-K4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(train_path)"
      ],
      "metadata": {
        "id": "9Jvu64RhHonP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(test_path)"
      ],
      "metadata": {
        "id": "8rBmCzwiHogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 3: Traffic Signs (GTSRB plus 162 custom classes)"
      ],
      "metadata": {
        "id": "hnpr15miBWfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = path_dataset3+\"Train/\"\n",
        "test_path = path_dataset3+\"Test/\""
      ],
      "metadata": {
        "id": "d0Xt8GP7-KmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(train_path)"
      ],
      "metadata": {
        "id": "cY3sLPQnBaZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_image_size_dist(test_path)"
      ],
      "metadata": {
        "id": "RUjDmWM_HuMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loaders**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IbmXHVFl-LrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path_train, val_split, path_test, batch_size, input_size):\n",
        "  \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    transform_train= transforms.Compose([transforms.Resize(input_size),\n",
        "                                          transforms.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.5, hue=0.3),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          normalize\n",
        "                                          ])\n",
        "\n",
        "    transform_test = transforms.Compose([transforms.Resize(input_size),\n",
        "                                         transforms.ToTensor(), \n",
        "                                         normalize])\n",
        "\n",
        "    data_train = datasets.ImageFolder(root=path_train, transform=transform_train)\n",
        "    data_test = datasets.ImageFolder(root=path_test, transform=transform_test)\n",
        "    \n",
        "    val_size = int(len(data_train)*val_split)\n",
        "    train_size = len(data_train) - val_size\n",
        "\n",
        "    train_dataset, val_dataset = td.random_split(data_train, [train_size, val_size])\n",
        "    \n",
        "    data_loader_train = td.DataLoader(train_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      drop_last=False,\n",
        "                                      num_workers=0,       \n",
        "                                      pin_memory=True) \n",
        "     \n",
        "    data_loader_val = td.DataLoader(val_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    drop_last=False,\n",
        "                                    num_workers=0)  \n",
        "      \n",
        "    data_loader_test = td.DataLoader(data_test,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   drop_last=False,\n",
        "                                   num_workers=0)\n",
        "    \n",
        "    return data_loader_train, data_loader_test, data_loader_val"
      ],
      "metadata": {
        "id": "nAjOrIQaaACu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-ystywhe-rXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nclasses=15"
      ],
      "metadata": {
        "id": "K4b2ZJVX_KEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "Zz46kldS13If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs, train_loader, criterion, optimizer, savepath):\n",
        "\n",
        "      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      print(\"Device: {}\".format(device))\n",
        "      model.to(device)\n",
        "\n",
        "      total_steps = len(train_loader)\n",
        "\n",
        "      t1 = time.time()\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          for i, data in enumerate(train_loader):\n",
        "              \n",
        "              images, labels = data[0].to(device), data[1].to(device)\n",
        "              \n",
        "              model.train()\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(images)\n",
        "              loss = criterion(outputs, labels)\n",
        "              \n",
        "              # Backprop and optimisation\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "              # Train accuracy\n",
        "              total = labels.size(0)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              correct = (predicted == labels).sum().item()\n",
        "              \n",
        "              if (i + 1) % 10 == 0:\n",
        "                  model.eval() \n",
        "                  with torch.no_grad(): \n",
        "                      correctv = 0\n",
        "                      totalv = 0\n",
        "                      for datav in val_loader:\n",
        "                          imagesv, labelsv = datav[0].to(device), datav[1].to(device)\n",
        "                          outputsv = model(imagesv)\n",
        "                          _, predictedv = torch.max(outputsv.data, 1)\n",
        "                          totalv += labelsv.size(0)\n",
        "                          correctv += (predictedv == labelsv).sum().item()\n",
        "                          \n",
        "                      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'\n",
        "                      .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
        "                          (correct / total) * 100,\n",
        "                          (correctv / totalv) * 100))\n",
        "                      \n",
        "                  \n",
        "      print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n",
        "\n",
        "      print(\"/n/n Saving model at: \",savepath)\n",
        "      torch.save(model.state_dict(), savepath)\n",
        "\n",
        "      return model, device"
      ],
      "metadata": {
        "id": "s9lpiTqe-y49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "   \n",
        "    model.eval() \n",
        "\n",
        "    Y=[]\n",
        "    y=[]\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            Y.extend(labels)\n",
        "            y.extend(predicted)\n",
        "\n",
        "        acc = (correct / total) * 100\n",
        "        f1 = F1Score(num_classes=nclasses)\n",
        "        f1score = f1(y, Y)\n",
        "\n",
        "        print('Model Evaluation Results on {} test samples'.format(total))\n",
        "        print('Test Accuracy: ', acc)\n",
        "        print('Test F1 Score: ',f1score)\n",
        "        "
      ],
      "metadata": {
        "id": "_mHsQq5I-yyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 1: AlexNet"
      ],
      "metadata": {
        "id": "jbDH531X-anJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', weights=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0C0mVSFA81I",
        "outputId": "c5193785-0b42-427d-9827-29d6fc373ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_out = AlexNet_model.classifier[4].out_features\n",
        "AlexNet_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ],
      "metadata": {
        "id": "gUhYh2APBCmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AlexNet_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTMSRbBQBF4h",
        "outputId": "3ae7977c-9395-4947-a741-d7910a5ba090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(AlexNet_model.parameters(), lr=learning_rate) "
      ],
      "metadata": {
        "id": "yKGRy1ztBL28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = path_dataset1+\"train\"\n",
        "path_test = path_dataset1+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 32\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ],
      "metadata": {
        "id": "dnyRBBLrs23v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_weights = [1/class_counts[i] for i in df.label.values]"
      ],
      "metadata": {
        "id": "ILaVUr5U4N2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savepath = path+\"alexnetmodel_d1.pt\"\n",
        "\n",
        "trained_AlexNet_model, device = train_model(AlexNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ],
      "metadata": {
        "id": "pzxEFoxxs26O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260ac207-57cc-470f-f9b4-b6699a0cc698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "Epoch [1/1], Step [10/66], Loss: 2.7568, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [20/66], Loss: 2.6704, Accuracy: 0.00%, Validation Accuracy: 10.33%\n",
            "Epoch [1/1], Step [30/66], Loss: 2.6873, Accuracy: 9.38%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [40/66], Loss: 2.6864, Accuracy: 15.62%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [50/66], Loss: 2.6760, Accuracy: 3.12%, Validation Accuracy: 9.18%\n",
            "Epoch [1/1], Step [60/66], Loss: 2.5726, Accuracy: 15.62%, Validation Accuracy: 15.30%\n",
            "######## Training Finished in 235.3470482826233 seconds ###########\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for loading model\n",
        "# trained_AlexNet_model.load_state_dict(torch.load(path+\"alexnetmodel.pt\"))"
      ],
      "metadata": {
        "id": "lBVfPGxuqTaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(trained_AlexNet_model, test_loader, device)"
      ],
      "metadata": {
        "id": "qnfIndd2s29e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4bbc6c-d292-48f7-ab90-c65c149acd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 710 test images: 19.859154929577468 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUru4h3JFq2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 2: VGG-11"
      ],
      "metadata": {
        "id": "-2Oy50W5vP6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG11_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', weights=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbQ_Vv-ZvS5D",
        "outputId": "e8d31eed-016d-4cc5-9515-d81db58fc832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_out = VGG11_model.classifier[3].out_features\n",
        "VGG11_model.classifier[6] = nn.Linear( prev_out, nclasses)"
      ],
      "metadata": {
        "id": "CICeGKI3vjDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG11_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2b6wbT-vy9o",
        "outputId": "00ac485b-9ba2-4d0c-f61c-878d21bae4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(VGG11_model.parameters(), lr=learning_rate) "
      ],
      "metadata": {
        "id": "lgOzrKXZv3DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = path_dataset1+\"train\"\n",
        "path_test = path_dataset1+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 32\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ],
      "metadata": {
        "id": "xQdrYPPhv_06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savepath = path+\"vgg11model_d1.pt\"\n",
        "trained_VGG11_model, device = train_model(VGG11_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ],
      "metadata": {
        "id": "1VBebkPiwIuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(trained_VGG11_model, test_loader, device)"
      ],
      "metadata": {
        "id": "pDj3U6BXwQYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 3: ResNet-18"
      ],
      "metadata": {
        "id": "x8QGSudN3rfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjeyUpF53r6L",
        "outputId": "3245f470-5281-4267-8f79-e8e23d7c8790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet_model.fc = nn.Linear( 512, nclasses)"
      ],
      "metadata": {
        "id": "jZz850Gk4P_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1niUp7-3ucX",
        "outputId": "46b7d327-13e0-4e2f-b744-5b28b279b71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate) "
      ],
      "metadata": {
        "id": "OAE_uMea3uY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = path_dataset1+\"train\"\n",
        "path_test = path_dataset1+\"test\"\n",
        "val_split = 0.2\n",
        "batch_size = 32\n",
        "input_size = (224,224)\n",
        "\n",
        "train_loader, test_loader, val_loader = load_data(path_train, val_split, path_test, batch_size, input_size)"
      ],
      "metadata": {
        "id": "YoNH0dSl3uVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savepath = path+\"resnetmodel_d1.pt\"\n",
        "trained_ResNet_model, device = train_model(ResNet_model, num_epochs, train_loader, criterion, optimizer, savepath)"
      ],
      "metadata": {
        "id": "alEPE-YH3uSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(trained_ResNet_model, test_loader, device)"
      ],
      "metadata": {
        "id": "9-r7XF9a3uPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}